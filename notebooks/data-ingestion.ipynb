{
  "metadata": {
    "name": "DataIngestion",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# CSCIGA 2437 Project: Data Ingestion\n## Chenmeinian Guo (cg3972)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Profiling"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.SparkSession\n\nval spark \u003d SparkSession.builder()\n  .appName(\"NYC Property Sales Analysis\")\n  .getOrCreate()\n\nval filePath \u003d \"./nyc-property-sales.csv\"\nval rawData \u003d spark.read\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .csv(filePath)\n\nrawData.printSchema()\nz.show(rawData)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val totalEntries \u003d rawData.count().toDouble\n\nval nullCounts \u003d rawData.columns.map { colName \u003d\u003e\n  val nullCount \u003d rawData.filter(col(colName).isNull).count()\n  val nullPercentage \u003d (nullCount / totalEntries) * 100\n  (colName, nullCount, nullPercentage)\n}\n\nprintln(\"Column-wise null value analysis:\")\nnullCounts.foreach { case (colName, nullCount, nullPercentage) \u003d\u003e\n  println(f\"$colName%-30s $nullCount%-10d ($nullPercentage%.2f%%)\")\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Cleaning"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// here we verified that the column \"EASE-MENT\" is useless because it\u0027s mostly either null or empty, only 14 entries are valid strings\nimport org.apache.spark.sql.functions._\n\nrawData.filter(col(\"EASE-MENT\").isNotNull).select(\"EASE-MENT\").distinct().show(5) // unique vals for EASE-MENT\n\nval countNullOrEmpty \u003d cleanedData.filter(col(\"EASE-MENT\").isNull || trim(col(\"EASE-MENT\")) \u003d\u003d\u003d \"\").count()\nval countNonNullOrEmpty \u003d cleanedData.filter(!(col(\"EASE-MENT\").isNull || trim(col(\"EASE-MENT\")) \u003d\u003d\u003d \"\")).count()\n\nprintln(s\"Number of null and empty values in EASE-MENT: $countNullOrEmpty\")\nprintln(s\"Number of string values in EASE-MENT: $countNonNullOrEmpty\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// a quick look on the columns that have null vals\ncleanedData.select(\"RESIDENTIAL UNITS\").show(5) \ncleanedData.select(\"COMMERCIAL UNITS\").show(5) \ncleanedData.select(\"TOTAL UNITS\").show(5)\ncleanedData.select(\"GROSS SQUARE FEET\").show(5)\ncleanedData.select(\"LAND SQUARE FEET\").show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/* \nclean dataset: \n    1. change \" ZIP CODE\" (with an unexpected space) to \"ZIP CODE\"\n    2. drop the few entries (e.g. \"ZIP CODE\") that have important null values\n    3. columns \"APARTMENT NUMBER\" and \"EASE-MENT\" are not useful for our analysis, so we simply drop it\n*/\nval cleanedData \u003d rawData\n                .withColumnRenamed(\" ZIP CODE\", \"ZIP CODE\")\n                .na.drop(Seq(\n                \"TAX CLASS AT PRESENT\", \"BUILDING CLASS AT PRESENT\",\n                \"ZIP CODE\",\n                \"YEAR BUILT\",\n                \"RESIDENTIAL UNITS\", \"COMMERCIAL UNITS\"\n                ))\n                .drop(\"APARTMENT NUMBER\")\n                .drop(\"EASE-MENT\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val nullCountsAfterCleansing \u003d cleanedData.columns.map { colName \u003d\u003e\n  val nullCount \u003d cleanedData.filter(col(colName).isNull).count()\n  val nullPercentage \u003d (nullCount / totalEntries) * 100\n  (colName, nullCount, nullPercentage)\n}\n\nprintln(\"Column-wise null value analysis:\")\nnullCountsAfterCleansing.foreach { case (colName, nullCount, nullPercentage) \u003d\u003e\n  println(f\"$colName%-30s $nullCount%-10d ($nullPercentage%.2f%%)\")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\n    while the column \"LAND SQUARE FEET\" is very important,\n    until this step, we\u0027ve checked there\u0027s just that one entry that has a null value \n    (which is \"LAND SQAURE FEET\"). Therefore, we can safely drop this entry without \n    significantly affecting our dataset\n*/\nval fullyCleanedData \u003d cleanedData\n                    .filter(col(\"LAND SQUARE FEET\").isNotNull)\n\n// we should only consider sucessfull sales (where column \"SALE PRICE\" is above zero)\n// but we found that too much data have zero for sales so we need to analyze it\nval cleanedDataRatio \u003d fullyCleanedData.filter($\"SALE PRICE\" \u003d\u003d\u003d 0).count() / totalEntries\nval totalCleanedEntries \u003d fullyCleanedData.count()\n\nprintln(\"Percentage of data reserved after cleaning: $cleanedDataRatio\")\nprintln(\"Remaining records after cleaning: $totalCleanedEntries\")"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Initially, I hypothesized that `SALE PRICE \u003d 0` entries might be mostly from the earlier years, such as 2003, due to incomplete records or less robust data collection practices at the time. However, the analysis shows that zero-sale cases occur consistently across all years, including 19,643 instances in 2021, though in declining trend. **(see below)**"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dataWithYear \u003d fullyCleanedData.withColumn(\n  \"YEAR\",\n  year(to_date(col(\"SALE DATE\"), \"yyyy-MM-dd HH:mm:ss\"))\n)\n\nval zeroSaleByYear \u003d dataWithYear\n  .filter(col(\"SALE PRICE\") \u003d\u003d\u003d 0)\n  .groupBy(\"YEAR\")\n  .agg(count(\"*\").alias(\"ZERO_SALE_COUNT\"))\n  .orderBy(desc(\"ZERO_SALE_COUNT\"))\n\nval zeroSalePercentageByYear \u003d dataWithYear\n  .filter(col(\"SALE PRICE\") \u003d\u003d\u003d 0)\n  .groupBy(\"YEAR\")\n  .agg(\n    count(\"*\").alias(\"ZERO_SALE_COUNT\"),\n    (count(\"*\") / fullyCleanedData.count() * 100).alias(\"PERCENTAGE_OF_TOTAL\")\n  )\n  .orderBy(desc(\"PERCENTAGE_OF_TOTAL\"))\n\nz.show(zeroSalePercentageByYear)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "After doing some research online, I noticed that entries with `SALE PRICE \u003d 0` often represent non-standard transactions such as donations, inheritances, family transfers, government acquisitions, or incomplete sales. These do not reflect typical market transactions and may skew price trend analyses. To improve accuracy, I removed all entries with SALE PRICE \u003d 0. However, analyzing these cases separately could reveal insights into specific property transfer patterns or anomalies, though the primary focus will remain on valid sales for investment analysis."
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// calculate mean and std\nval stats \u003d fullyCleanedData.select(mean(\"SALE PRICE\"), stddev(\"SALE PRICE\")).first()\nval meanPrice \u003d stats.getDouble(0)\nval stdDevPrice \u003d stats.getDouble(1)\n\n// remove all entries where the sale price is zero\nval filteredData \u003d fullyCleanedData.filter(col(\"SALE PRICE\") !\u003d\u003d 0)\n\nprintln(s\"Remaining records after filtering: ${filteredData.count()}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(filteredData)"
    }
  ]
}