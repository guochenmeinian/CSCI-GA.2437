{
  "metadata": {
    "name": "CSCIGA 2437",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# CSCIGA 2437 Project (Real Estate)\n## Chenmeinian Guo (cg3972)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data Profiling"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.SparkSession\n\nval spark \u003d SparkSession.builder()\n  .appName(\"NYC Property Sales Analysis\")\n  .getOrCreate()\n\nval filePath \u003d \"./nyc-property-sales.csv\"\nval rawData \u003d spark.read\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .csv(filePath)\n\nrawData.printSchema()\nz.show(rawData)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val totalEntries \u003d rawData.count().toDouble\n\nval nullCounts \u003d rawData.columns.map { colName \u003d\u003e\n  val nullCount \u003d rawData.filter(col(colName).isNull).count()\n  val nullPercentage \u003d (nullCount / totalEntries) * 100\n  (colName, nullCount, nullPercentage)\n}\n\nprintln(\"Column-wise null value analysis:\")\nnullCounts.foreach { case (colName, nullCount, nullPercentage) \u003d\u003e\n  println(f\"$colName%-30s $nullCount%-10d ($nullPercentage%.2f%%)\")\n}"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data Cleaning"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// here we verified that the column \"EASE-MENT\" is useless because it\u0027s mostly either null or empty, only 14 entries are valid strings\nimport org.apache.spark.sql.functions._\n\nrawData.filter(col(\"EASE-MENT\").isNotNull).select(\"EASE-MENT\").distinct().show(5) // unique vals for EASE-MENT\n\nval countNullOrEmpty \u003d rawData.filter(col(\"EASE-MENT\").isNull || trim(col(\"EASE-MENT\")) \u003d\u003d\u003d \"\").count()\nval countNonNullOrEmpty \u003d rawData.filter(!(col(\"EASE-MENT\").isNull || trim(col(\"EASE-MENT\")) \u003d\u003d\u003d \"\")).count()\n\nprintln(s\"Number of null and empty values in EASE-MENT: $countNullOrEmpty\")\nprintln(s\"Number of string values in EASE-MENT: $countNonNullOrEmpty\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// a quick look on the columns that have null vals\nrawData.select(\"RESIDENTIAL UNITS\").show(5) \nrawData.select(\"COMMERCIAL UNITS\").show(5) \nrawData.select(\"TOTAL UNITS\").show(5)\nrawData.select(\"GROSS SQUARE FEET\").show(5)\nrawData.select(\"LAND SQUARE FEET\").show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/* \nclean dataset: \n    1. change \" ZIP CODE\" (with an unexpected space) to \"ZIP CODE\"\n    2. drop the few entries (e.g. \"ZIP CODE\") that have important null values\n    3. columns \"APARTMENT NUMBER\" and \"EASE-MENT\" are not useful for our analysis, so we simply drop it\n    4. some column names have extra space (i.e. category_type), so we extract the index to make sure there\u0027s no duplicate\n*/\nval cleanedData \u003d rawData\n                .withColumnRenamed(\" ZIP CODE\", \"ZIP CODE\")\n                .na.drop(Seq(\n                \"TAX CLASS AT PRESENT\", \"BUILDING CLASS AT PRESENT\",\n                \"ZIP CODE\",\n                \"YEAR BUILT\",\n                \"RESIDENTIAL UNITS\", \"COMMERCIAL UNITS\"\n                ))\n                .drop(\"APARTMENT NUMBER\")\n                .drop(\"EASE-MENT\")\n                .withColumn(\n                \"CATEGORY_TYPE\",\n                regexp_extract(col(\"BUILDING CLASS CATEGORY\"), \"^\\\\d+\", 0) // extract category index\n                )\n                .withColumn(\n                \"CATEGORY_FULL\",\n                regexp_replace(trim(col(\"BUILDING CLASS CATEGORY\")), \"^\\\\d+\\\\s*\", \"\") // extract category name\n                )"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val nullCountsAfterCleansing \u003d cleanedData.columns.map { colName \u003d\u003e\n  val nullCount \u003d cleanedData.filter(col(colName).isNull || col(colName) \u003d\u003d\u003d \"\").count()\n  val nullPercentage \u003d (nullCount / totalEntries) * 100\n  (colName, nullCount, nullPercentage)\n}\n\nprintln(\"Column-wise null value analysis:\")\nnullCountsAfterCleansing.foreach { case (colName, nullCount, nullPercentage) \u003d\u003e\n  println(f\"$colName%-30s $nullCount%-10d ($nullPercentage%.2f%%)\")\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "/*\n    LAND SQUARE FEET and CATEGORY_TYPE are important,\n    and we\u0027ve checked there\u0027re only a few data points that have a null/empty value \n    left. Therefore, we can safely drop these entries without \n    significantly affecting our dataset\n    \n    I later found that there\u0027re zip code \u003d\u003d 0.0, so we need to get rid of this as well\n    \n    Here I used fullyCleanedData to analyze the cases where sale price equal to null/zero, whereas filteredData all have a valid sale price.\n*/\nval fullyCleanedData \u003d cleanedData\n                    .filter(col(\"LAND SQUARE FEET\").isNotNull)\nval filteredData \u003d cleanedData.filter(col(\"CATEGORY_TYPE\").isNotNull \u0026\u0026 col(\"CATEGORY_TYPE\") \u003d!\u003d \"\")\n                    .filter(col(\"CATEGORY_FULL\").isNotNull \u0026\u0026 col(\"CATEGORY_FULL\") \u003d!\u003d \"\")\n                    .filter(col(\"ZIP CODE\").isNotNull \u0026\u0026 col(\"CATEGORY_FULL\") \u003d!\u003d 0)\n\n// we should only consider sucessfull sales (where column \"SALE PRICE\" is above zero)\n// but we found that too much data have zero for sales so we need to analyze it\nval cleanedDataRatio \u003d fullyCleanedData.filter($\"SALE PRICE\" \u003d\u003d\u003d 0).count() / totalEntries\nval totalCleanedEntries \u003d fullyCleanedData.count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Initially, I hypothesized that `SALE PRICE \u003d 0` entries might be mostly from the earlier years, such as 2003, due to incomplete records or less robust data collection practices at the time. However, the analysis shows that zero-sale cases occur consistently across all years, including 19,643 instances in 2021, though in declining trend. **(see below)**"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dataWithYear \u003d fullyCleanedData.withColumn(\n  \"YEAR\",\n  year(to_date(col(\"SALE DATE\"), \"yyyy-MM-dd HH:mm:ss\"))\n)\n\nval zeroSaleByYear \u003d dataWithYear\n  .filter(col(\"SALE PRICE\") \u003d\u003d\u003d 0)\n  .groupBy(\"YEAR\")\n  .agg(count(\"*\").alias(\"ZERO_SALE_COUNT\"))\n  .orderBy(desc(\"ZERO_SALE_COUNT\"))\n\nval zeroSalePercentageByYear \u003d dataWithYear\n  .filter(col(\"SALE PRICE\") \u003d\u003d\u003d 0)\n  .groupBy(\"YEAR\")\n  .agg(\n    count(\"*\").alias(\"ZERO_SALE_COUNT\"),\n    (count(\"*\") / fullyCleanedData.count() * 100).alias(\"PERCENTAGE_OF_TOTAL\")\n  )\n  .orderBy(desc(\"PERCENTAGE_OF_TOTAL\"))\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "I noticed a substantial amount of zero sale entries. Since these do not reflect typical market transactions and may skew price trend analyses, to improve accuracy, I removed all entries with `SALE PRICE` \u003d 0. "
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// calculate mean and std\nval stats \u003d dataWithYear.select(mean(\"SALE PRICE\"), stddev(\"SALE PRICE\")).first()\nval meanPrice \u003d stats.getDouble(0)\nval stdDevPrice \u003d stats.getDouble(1)\n\n// remove all entries where the sale price is zero\nval zeroSaleData \u003d dataWithYear.filter(col(\"SALE PRICE\") !\u003d\u003d 0)\n\nprintln(s\"Remaining records after filtering: ${filteredData.count()}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(dataWithYear)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(fullyCleanedData) // this is the real fully cleaned data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data Analysis"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Zero Sale Analysis\n\nAfter doing some research online, I found that entries with `SALE PRICE \u003d 0` often represent non-standard transactions such as donations, inheritances, family transfers, government acquisitions, or incomplete sales. Analyzing these zero sale cases separately might reveal insights into specific property transfer patterns or anomalies, though the primary focus will remain on valid sales for investment analysis. Here I performed some simple analysis below."
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val zeroSaleByYear \u003d dataWithYear\n  .filter(col(\"SALE PRICE\") \u003d\u003d\u003d 0)\n  .groupBy(\"YEAR\")\n  .agg(count(\"*\").alias(\"ZERO_SALE_COUNT\"))\n  .orderBy(\"YEAR\") // sort by year\n\nz.show(zeroSaleByYear)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val zeroSaleByNeighborhood \u003d dataWithYear\n  .filter(col(\"SALE PRICE\") \u003d\u003d\u003d 0)\n  .groupBy(\"NEIGHBORHOOD\")\n  .agg(count(\"*\").alias(\"ZERO_SALE_COUNT\"))\n  .orderBy(desc(\"ZERO_SALE_COUNT\")) // sort by sale amount\n\nz.show(zeroSaleByNeighborhood)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This focuses on neighborhoods in New York City with significant counts of zero-sale transactions—property transfers without a monetary sale price. These often indicate non-standard transactions like inheritances, donations, family transfers, or government acquisitions. For example, neighborhoods like Midtown West (12,217) and Flushing-North (11,537) lead in zero-sale counts, potentially reflecting high activity in corporate or institutional transfers. In contrast, areas like Bedford Stuyvesant (8,930) and Crown Heights (5,160) might reflect more localized, community-driven property exchanges, such as inheritances or family transfers.\n\nUnderstanding these patterns helps identify where such non-market activities are concentrated, providing insights into urban property trends and socio-economic behaviors within different neighborhoods."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Category"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This analysis tracks the price trends over time and the percentage growth for various categories. The first table highlights the yearly average, median sale prices, and total sales across all categories. The second table focuses on comparing the overall growth rates of these categories and is sorted accordingly.\n\n**Findings from the first table**:\n\n1. Total sales:\n   - Coops elevator apartments\n   - Condos elevator apartments\n   - One family dwellings\n   - Walkup apartments\n   - Three family dwellings\n\n2. Median sale price:\n   - Theatres\n   - Rentals elevator apartments\n   - Loft buildings\n   - Condos elevator apartments\n   - Hospital and health facilities\n\n3. Average sale price:\n   - Theatres\n   - A condo rental\n   - Office buildings\n   - Tax class 4 utility bureau properties\n   - Tax class 3 utility properties\n\n**Some Background knowledge that might help for those who don\u0027t know**:\n\n- **Coops vs. condos**:\n  - Coops: Buyers own shares in a corporation that grants rights to occupy a unit. Usually involves restrictions on renting and board approval.\n  - Condos: Buyers own the unit outright and share common areas. Offers more flexibility in renting.\n\n- **One family dwellings vs. family homes**:\n  - Generally refer to standalone properties for single-family use. Differences may arise from regional naming conventions or slight design variations.\n\n- **Tax classes**:\n  - Tax class 3: Includes utility properties like power generation facilities.\n  - Tax class 4: Covers commercial and industrial properties, such as office buildings or warehouses.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// sum of median for each group\nval categoryStats \u003d fullyCleanedData.groupBy(\"CATEGORY_TYPE\", \"CATEGORY_FULL\")\n  .agg(\n    avg(\"SALE PRICE\").alias(\"AVERAGE_SALE_PRICE\"),\n    expr(\"percentile_approx(`SALE PRICE`, 0.5)\").alias(\"MEDIAN_SALE_PRICE\"),\n    count(\"*\").alias(\"TOTAL_SALES\")\n  )\n  .orderBy(desc(\"AVERAGE_SALE_PRICE\"))\n\nz.show(categoryStats)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "For the rest of the code,\n\n- **First Table (yearlyGrowth)**: Tracks yearly growth rates for each category based on the percentage change in average sale prices compared to the previous year. This table provides an overview of the processed data used for analysis.\n\n- **Second Table (categoryGrowthRate)**: Aggregates growth rates to calculate the overall average growth rate across all years for each category. This table identifies categories with consistently high growth rates. However, there are some issues with this table due to outliers, such as the first category, `Government Facilities`, which shows an extremely large growth rate of `6.6E7`. Because of this, a graph is not drawn for this table.\n\n- **Third Table (yearlyStatsWithZip)**: Provides insights into sale price trends at the ZIP code level. For example, the top three ZIP codes are `10020`, `10038`, and `10024`, corresponding to the Midtown Rockefeller Center area and Wall Street area. This reflects the potential of building prices in these regions.\n\n- **Fourth Table (yearlyStatsWithZip)**: Provides insights into sale price trends at the ZIP code level for each category. For example, the most expansive category is commerical condos at `10020`.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\n\nval enrichedData \u003d fullyCleanedData.withColumn(\"YEAR\", year(to_date(col(\"SALE DATE\"), \"yyyy-MM-dd HH:mm:ss\")))\n\nval yearlyStats \u003d enrichedData.groupBy(\"CATEGORY_FULL\", \"YEAR\")\n  .agg(avg(\"SALE PRICE\").alias(\"AVERAGE_SALE_PRICE\"), count(\"*\").alias(\"TOTAL_SALES\"))\n  .orderBy(\"CATEGORY_FULL\", \"YEAR\")\n\nval windowSpec \u003d Window.partitionBy(\"CATEGORY_FULL\").orderBy(\"YEAR\")\n\nval yearlyGrowth \u003d yearlyStats\n  .withColumn(\"PREV_YEAR_AVERAGE\", lag(\"AVERAGE_SALE_PRICE\", 1).over(windowSpec))\n  .withColumn(\"GROWTH_RATE\", (col(\"AVERAGE_SALE_PRICE\") - col(\"PREV_YEAR_AVERAGE\")) / col(\"PREV_YEAR_AVERAGE\") * 100)\n  .filter(col(\"GROWTH_RATE\").isNotNull)\n\nz.show(yearlyGrowth)\n\nval categoryGrowthRate \u003d yearlyGrowth.groupBy(\"CATEGORY_FULL\")\n  .agg(avg(\"GROWTH_RATE\").alias(\"AVERAGE_GROWTH_RATE\"), count(\"*\").alias(\"YEARS_WITH_DATA\"))\n  .orderBy(desc(\"AVERAGE_GROWTH_RATE\"))\n\nz.show(categoryGrowthRate)\n\nval yearlyStatsWithZip \u003d enrichedData.groupBy(\"CATEGORY_FULL\", \"YEAR\", \"ZIP CODE\")\n  .agg(avg(\"SALE PRICE\").alias(\"AVERAGE_SALE_PRICE\"), count(\"*\").alias(\"TOTAL_SALES\"))\n  .orderBy(\"CATEGORY_FULL\", \"ZIP CODE\", \"YEAR\")\n\nz.show(yearlyStatsWithZip) // for zip\nz.show(yearlyStatsWithZip) // for zip and category\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Neighborhood"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This analysis evaluates neighborhoods based on their growth rates, average sale prices, median sale prices, and total sales:\n\n1. **Growth Rate**: \n   - The `neighborhoodGrowthRate` table identifies neighborhoods with the highest average annual growth rates (sorted in descending order). A particular outlier is the `Clinton` neighborhood, where the growth rate was unexpectedly high. Further inspection confirmed this large number due to specific data points (some with growth rate as high as 1E7).\n\n2. **Sale Price and Volume**: \n   - The `neighborhoodAnalysis` table provides insights into the average and median sale prices, as well as total sales for each neighborhood. This helps pinpoint high-value neighborhoods and those with significant market activity. For example, neighborhoods with high average sale prices may indicate premium real estate markets, while high total sales reflect areas with greater transaction activity.\n\nThis analysi overall is similar to the previous category analysis.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val neighborhoodGrowthRate \u003d yearlyGrowth\n  .withColumn(\"GROWTH_RATE\", col(\"GROWTH_RATE\") / 100) \n  .groupBy(\"NEIGHBORHOOD\") // group by neighborhood\n  .agg(\n    avg(\"GROWTH_RATE\").alias(\"AVERAGE_GROWTH_RATE\") // average growth rate\n  )\n  .withColumn(\"AVERAGE_GROWTH_RATE\", col(\"AVERAGE_GROWTH_RATE\") * 100)\n  .orderBy(desc(\"AVERAGE_GROWTH_RATE\"))\n\nz.show(neighborhoodGrowthRate)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// here I felt little confused how can the average growth rate (e.g. this CLINTON neighborhood) be such a large number but after checking the neighborhood below I found that the growth rate are indeed that large.\nz.show(filteredData\n  .filter(col(\"NEIGHBORHOOD\") \u003d\u003d\u003d \"CLINTON\")\n  .orderBy(desc(\"GROWTH_RATE\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Group by NEIGHBORHOOD and calculate stats\nval neighborhoodAnalysis \u003d filteredData.groupBy(\"NEIGHBORHOOD\")\n  .agg(\n    avg(\"SALE PRICE\").alias(\"AVERAGE_SALE_PRICE\"),       // average sales\n    expr(\"percentile_approx(`SALE PRICE`, 0.5)\").alias(\"MEDIAN_SALE_PRICE\"), // median sales\n    count(\"*\").alias(\"TOTAL_SALES\")                     // total sales\n  )\n  .orderBy(desc(\"AVERAGE_SALE_PRICE\"))\n\nz.show(neighborhoodAnalysis)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Time"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "I analyzed the data based on time, focusing on months and quarters:\n\n- Monthly Analysis: \n  - In terms of average sales, months `1`, `3`, `6`, and `12` show relatively high performance, with `12` (December) standing out as the highest. \n  - For total sales, mid-year months like `6` (June) and `8` (August) lead the pack, while December also shows strong performance.\n\n- Quarterly Analysis: \n  - For average sales, the quarters exhibit consistent figures: Q1 (24%), Q2 (2%5), Q3 (24%), and Q4 (27%), with Q4 slightly outperforming the others. \n  - In terms of total sales, Q3 (26%) shows the highest volume, followed closely by Q2 (25%) and Q4 (25%), while Q1 (23%) lags behind. "
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Compare Average \u0026 Total Sale Price for different months\nval monthlyStats \u003d enrichedData\n  .withColumn(\"MONTH\", month(to_date(col(\"SALE DATE\"), \"yyyy-MM-dd HH:mm:ss\")))\n  .groupBy(\"MONTH\")\n  .agg(\n    avg(\"SALE PRICE\").alias(\"AVERAGE_SALE_PRICE\"),\n    count(\"*\").alias(\"TOTAL_SALES\")\n  )\n  .orderBy(\"MONTH\")\n\nz.show(monthlyStats)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Extract quarter from sale date\nval quarterlyStats \u003d enrichedData\n  .withColumn(\"QUARTER\", quarter(to_date(col(\"SALE DATE\"), \"yyyy-MM-dd HH:mm:ss\")))\n  .groupBy(\"YEAR\", \"QUARTER\")\n  .agg(\n    avg(\"SALE PRICE\").alias(\"AVERAGE_SALE_PRICE\"),       // average quarter sale\n    count(\"*\").alias(\"TOTAL_SALES\")                     // total sales\n  )\n  .orderBy(\"YEAR\", \"QUARTER\")\n\nz.show(quarterlyStats)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Group by neighborhood and year to compute yearly average sale prices and total sales\nval neighborhoodYearlyStats \u003d enrichedData\n  .groupBy(\"NEIGHBORHOOD\", \"YEAR\")\n  .agg(\n    avg(\"SALE PRICE\").alias(\"AVERAGE_SALE_PRICE\"),\n    count(\"*\").alias(\"TOTAL_SALES\")\n  )\n  .orderBy(\"NEIGHBORHOOD\", \"YEAR\")\n\n// Define a window for calculating growth rate by neighborhood\nval neighborhoodWindowSpec \u003d Window.partitionBy(\"NEIGHBORHOOD\").orderBy(\"YEAR\")\n\n// Compute the growth rate for each neighborhood\nval neighborhoodGrowth \u003d neighborhoodYearlyStats\n  .withColumn(\"PREV_YEAR_AVERAGE\", lag(\"AVERAGE_SALE_PRICE\", 1).over(neighborhoodWindowSpec))\n  .withColumn(\n    \"GROWTH_RATE\",\n    (col(\"AVERAGE_SALE_PRICE\") - col(\"PREV_YEAR_AVERAGE\")) / col(\"PREV_YEAR_AVERAGE\") * 100\n  )\n  .filter(col(\"GROWTH_RATE\").isNotNull) // Exclude null growth rates\n\nz.show(neighborhoodGrowth)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Clustering"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "This analysis identifies patterns in property categories by clustering based on **average sale price**, **growth rate**, and **total sales**. The data was preprocessed to remove outliers (1st and 99th percentiles) and standardized for better performance. Principal Component Analysis (PCA) reduced the features to two dimensions for visualization.\n\n## Clustering Process\n\nUsing KMeans with `k\u003d5`, clusters were created to group property categories with similar price and growth patterns. Each cluster’s characteristics, such as average sale price, growth rate, and dominant property categories, were analyzed. The centroids of clusters indicate the central tendencies for the features.\n\n## Key Findings\n\n- **Cluster Characteristics**: Each cluster represents a distinct market behavior, grouping categories with similar price and growth trends. \n- **High-Growth Categories**: Clusters with the highest growth rate centroids were identified, highlighting categories with the most promising market potential.\n- **Category Distribution**: For each cluster, the dominant property categories and their average statistics reveal insights into market segmentation.\n- **Visualization**: PCA-based cluster visualization shows clear separation and clustering of property categories in reduced dimensions.\n\n## Results and Implications\n\nThis analysis provides actionable insights by highlighting high-growth property categories and segmenting the market into distinct groups. Cluster centroids summarize the tendencies of each group, offering a foundation for targeted decision-making in real estate investment.\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.ml.feature.{VectorAssembler, StandardScaler, PCA}\nimport org.apache.spark.sql.functions._\n\n// Step 1: Prepare data with additional features\nval clusteringData \u003d yearlyGrowth\n  .filter(col(\"GROWTH_RATE\").isNotNull \u0026\u0026 col(\"AVERAGE_SALE_PRICE\").isNotNull)\n  .select(\"CATEGORY_FULL\", \"AVERAGE_SALE_PRICE\", \"GROWTH_RATE\", \"TOTAL_SALES\")\n\n// Remove outliers using 1st and 99th percentile thresholds\nval priceStats \u003d clusteringData.stat.approxQuantile(\"AVERAGE_SALE_PRICE\", Array(0.01, 0.99), 0.0)\nval growthStats \u003d clusteringData.stat.approxQuantile(\"GROWTH_RATE\", Array(0.01, 0.99), 0.0)\nval totalSalesStats \u003d clusteringData.stat.approxQuantile(\"TOTAL_SALES\", Array(0.01, 0.99), 0.0)\n\nval filteredClusteringData \u003d clusteringData\n  .filter(col(\"AVERAGE_SALE_PRICE\").between(priceStats(0), priceStats(1)))\n  .filter(col(\"GROWTH_RATE\").between(growthStats(0), growthStats(1)))\n  .filter(col(\"TOTAL_SALES\").between(totalSalesStats(0), totalSalesStats(1)))\n\n// Step 2: Assemble features and scale\nval assembler \u003d new VectorAssembler()\n  .setInputCols(Array(\"AVERAGE_SALE_PRICE\", \"GROWTH_RATE\", \"TOTAL_SALES\"))\n  .setOutputCol(\"rawFeatures\")\n\nval assembledData \u003d assembler.transform(filteredClusteringData)\n\nval scaler \u003d new StandardScaler()\n  .setInputCol(\"rawFeatures\")\n  .setOutputCol(\"features\")\n  .setWithMean(true)\n  .setWithStd(true)\n\nval scaledData \u003d scaler.fit(assembledData).transform(assembledData)\n\n// Step 3: Apply PCA for dimensionality reduction\nval pca \u003d new PCA()\n  .setInputCol(\"features\")\n  .setOutputCol(\"pcaFeatures\")\n  .setK(2) // Reduce to 2 dimensions for visualization\nval pcaModel \u003d pca.fit(scaledData)\nval pcaData \u003d pcaModel.transform(scaledData)\n\n// Step 4: KMeans clustering\nval kmeans \u003d new KMeans()\n  .setK(5)\n  .setSeed(1L)\n  .setFeaturesCol(\"pcaFeatures\")\n  .setPredictionCol(\"cluster\")\n\nval kmeansModel \u003d kmeans.fit(pcaData)\nval clusteredData \u003d kmeansModel.transform(pcaData)\n\n// Step 5: Analyze cluster composition\nval clusterComposition \u003d clusteredData.groupBy(\"cluster\")\n  .agg(\n    count(\"*\").alias(\"COUNT\"),\n    avg(\"AVERAGE_SALE_PRICE\").alias(\"AVG_SALE_PRICE\"),\n    avg(\"GROWTH_RATE\").alias(\"AVG_GROWTH_RATE\"),\n    collect_list(\"CATEGORY_FULL\").alias(\"CATEGORY_LIST\")\n  )\n  .orderBy(\"cluster\")\n\nz.show(clusterComposition)\n\n// Step 6: Display cluster centroids\nval centroids \u003d kmeansModel.clusterCenters.zipWithIndex.map { case (center, idx) \u003d\u003e\n  (idx, center(0), center(1))\n}.toSeq.toDF(\"Cluster\", \"Centroid_1\", \"Centroid_2\")\n\nz.show(centroids)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(clusteredData.select(\"CATEGORY_FULL\", \"AVERAGE_SALE_PRICE\", \"GROWTH_RATE\", \"cluster\")\n  .orderBy(\"cluster\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Step 1: Analyze each cluster\u0027s composition\nval clusterComposition \u003d clusteredData.groupBy(\"cluster\")\n  .agg(\n    count(\"*\").alias(\"COUNT\"), // Total count of data points in each cluster\n    avg(\"AVERAGE_SALE_PRICE\").alias(\"AVG_SALE_PRICE\"), // Average sale price in each cluster\n    avg(\"GROWTH_RATE\").alias(\"AVG_GROWTH_RATE\"), // Average growth rate in each cluster\n    collect_list(\"CATEGORY_FULL\").alias(\"CATEGORY_LIST\") // List of property categories in each cluster\n  )\n  .orderBy(\"cluster\")\n\nz.show(clusterComposition)\n\n// Step 2: Analyze cluster distribution by CATEGORY\nval clusterNeighborhoodAnalysis \u003d clusteredData.groupBy(\"cluster\", \"CATEGORY_FULL\")\n  .agg(\n    count(\"*\").alias(\"CATEGORY_COUNT\"), // Total count per category in each cluster\n    avg(\"AVERAGE_SALE_PRICE\").alias(\"AVG_SALE_PRICE_CATEGORY\"), // Average sale price per category\n    avg(\"GROWTH_RATE\").alias(\"AVG_GROWTH_RATE_CATEGORY\") // Average growth rate per category\n  )\n  .orderBy(col(\"cluster\"), col(\"CATEGORY_COUNT\").desc) // Fixed sorting logic\n\nz.show(clusterNeighborhoodAnalysis)\n\n// Step 3: Extract centroids for each cluster\nval clusterCentroids \u003d kmeansModel.clusterCenters.zipWithIndex.map { case (center, idx) \u003d\u003e\n  (idx, center.toArray(0), center.toArray(1)) // Assuming 2D features: sale price and growth rate\n}.toSeq\n\nval centroidsDF \u003d spark.createDataFrame(clusterCentroids).toDF(\"Cluster\", \"Centroid_Sale_Price\", \"Centroid_Growth_Rate\")\n\nz.show(centroidsDF)\n\n// Step 4: Investigate clusters with the highest growth rates\nval highGrowthClusters \u003d clusteredData\n  .filter(col(\"cluster\") \u003d\u003d\u003d clusterCentroids.maxBy(_._3)._1) // Find the cluster with the highest growth rate centroid\n  .groupBy(\"CATEGORY_FULL\")\n  .agg(\n    avg(\"GROWTH_RATE\").alias(\"AVG_GROWTH_RATE\"), // Average growth rate for this cluster\n    avg(\"AVERAGE_SALE_PRICE\").alias(\"AVG_SALE_PRICE\") // Average sale price for this cluster\n  )\n  .orderBy(desc(\"AVG_GROWTH_RATE\"))\n\nz.show(highGrowthClusters)\n\n// Step 5: Visualize cluster results\nval visualizationData \u003d clusteredData.select(\"cluster\", \"AVERAGE_SALE_PRICE\", \"GROWTH_RATE\")\n\nz.show(visualizationData)\n\n"
    }
  ]
}